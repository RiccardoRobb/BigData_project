{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiccardoRobb/BigData_project/blob/main/BigData_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydnT0-krmCTZ"
      },
      "source": [
        "# **Twitter sentiment analysis**\n",
        "### BigData 2023 project\n",
        "\n",
        "[@Author](https://github.com/RiccardoRobb): Riccardo Ruberto 1860609"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaC0pFVy5ha-"
      },
      "source": [
        "---\n",
        "## **Inital configuration**\n",
        "### PySpark installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9pfBiCD4wMp",
        "outputId": "227babb5-0ef2-433c-e9d9-96184b864f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "! pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5rRWp3R5xKn"
      },
      "source": [
        "### Useful imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "DZyT2VD7ZMb6"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMZu4CcD51-e"
      },
      "source": [
        "### Spark configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Zc_wOE8Fl7aN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "9abc46fc-4315-4f65-f4ac-f5957e78194d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionRefusedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-f4e68ad72b89>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create the session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                 \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.ui.port'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"8000\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.executor.memory'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'4G'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.driver.memory'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'45G'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/conf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loadDefaults, _jvm, _jconf)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;31m# JVM is created, so create self._jconf directly through JVM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadDefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1710\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mUserHelpAutoCompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m         answer = self._gateway_client.send_command(\n\u001b[0m\u001b[1;32m   1713\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFLECTION_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
          ]
        }
      ],
      "source": [
        "# Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"8000\").\\\n",
        "                set('spark.executor.memory', '4G').\\\n",
        "                set('spark.driver.memory', '45G').\\\n",
        "                set('spark.driver.maxResultSize', '10G').\\\n",
        "                setAppName(\"Twitter sentiment analysis\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJSs9ejr58TM"
      },
      "source": [
        "### Check spark configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rHsaeM75DvA"
      },
      "outputs": [],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTx8THde5L9T"
      },
      "outputs": [],
      "source": [
        "sc._conf.getAll()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrHRiI0C-jDI"
      },
      "source": [
        "---\n",
        "## Load dataset [***Sentiment140***]\n",
        "### Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zayTBDQL98iG"
      },
      "outputs": [],
      "source": [
        "! wget https://raw.githubusercontent.com/RiccardoRobb/BigData_project/main/Sentiment140.zip\n",
        "\n",
        "! unzip \"./*.zip\" && rm *.zip\n",
        "! mv training.1600000.processed.noemoticon.csv train140.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAbLzmUGXNu_"
      },
      "source": [
        "### Create data frame [***Sentiment140***]\n",
        "1600000 tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7UAiEc-_GEv"
      },
      "outputs": [],
      "source": [
        "schema = StructType([ \\\n",
        "    StructField(\"target\",IntegerType(),True), \\\n",
        "    StructField(\"id\",LongType(),True), \\\n",
        "    StructField(\"full_date\",StringType(),True), \\\n",
        "    StructField(\"flag\", StringType(), True), \\\n",
        "    StructField(\"user\", StringType(), True), \\\n",
        "    StructField(\"text\", StringType(), True) \\\n",
        "  ])\n",
        "\n",
        "df = spark.read.csv('./train140.csv', schema=schema, header=\"false\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUGsEwMyXR9d"
      },
      "source": [
        "---\n",
        "## **Inital cleaning**\n",
        "### Removal of unnecessary columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfTVRLBRXTDx"
      },
      "outputs": [],
      "source": [
        "# sentiment of the tweet is not affected by the \"user\"\n",
        "df = df.drop(\"user\")\n",
        "\n",
        "# verify \"flag\" utility\n",
        "print(df.select(countDistinct(\"flag\")).collect()[0][0])\n",
        "\n",
        "# \"flag\" has only one value == NO_QUERY, so I delete it\n",
        "df = df.drop(\"flag\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efsWAcIVXjxt"
      },
      "source": [
        "### From ***date*** to ***day_name***; ***hour***; ***date***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnXFsS1rXsjI"
      },
      "outputs": [],
      "source": [
        "months_map = {\"Jan\": \"01\", \"Feb\": \"02\", \"Mar\": \"03\", \"Apr\": \"04\", \"May\": \"05\", \"Jun\": \"06\", \"Jul\": \"07\", \"Aug\": \"08\", \"Sep\": \"09\", \"Oct\": \"10\", \"Nov\": \"11\", \"Dec\": \"12\"}\n",
        "\n",
        "convert_date_udf = udf(lambda month_name : months_map[month_name], StringType())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUg1WK_Ml0Au"
      },
      "source": [
        "### From **hour:min:mill** to int **hour**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrCqebf9l9uP"
      },
      "outputs": [],
      "source": [
        "convert_hour_udf = udf(lambda time: int(time[:2]), IntegerType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcXyz4aKg_ul"
      },
      "outputs": [],
      "source": [
        "split_col = pyspark.sql.functions.split(df['full_date'], ' ')\n",
        "\n",
        "df = df.withColumn(\"day_name\", split_col.getItem(0)) \\\n",
        "      .withColumn(\"hour\", convert_hour_udf(split_col.getItem(3))) \\\n",
        "      .withColumn(\"date\", to_date( concat_ws(\"-\", split_col.getItem(2), convert_date_udf(split_col.getItem(1)), split_col.getItem(5)), \"dd-MM-yyyy\"))\n",
        "\n",
        "df = df.drop(\"full_date\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CyYJJi59XKs"
      },
      "source": [
        "---\n",
        "## **Data analysis**\n",
        "### ***target*** values analisys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1JWoNnM9k4k"
      },
      "outputs": [],
      "source": [
        "#df.select(\"target\").distinct().show()\n",
        "# \"target\" value is or 0 or 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehZD6TQNAH6H"
      },
      "outputs": [],
      "source": [
        "sad_tweets = df.filter(col(\"target\") == 0)\n",
        "happy_tweets = df.filter(col(\"target\") == 4)\n",
        "\n",
        "print(\"Sad tweets = \", sad_tweets.count())\n",
        "print(\"Happy tweets = \", happy_tweets.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n13xetpApkt"
      },
      "source": [
        "Sad tweets and happy tweets are balanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW2CBtpkBIGM"
      },
      "source": [
        "### Time frame of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNQmo6WsBQDD"
      },
      "outputs": [],
      "source": [
        "print(\"Min date = \", df.select(min(df.date)).collect()[0][0])\n",
        "print(\"Max date = \", df.select(max(df.date)).collect()[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PXQSmfOCezG"
      },
      "source": [
        "The time frame used is too small, datas were collected in 2 months. \\\\\n",
        "*Using data column will be useful only if we try to predict tweets written during the [2009-04-06, 2009-06-25] period.*\n",
        "\n",
        "### Better to delete the ***date*** column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwpxRRdnDZrI"
      },
      "outputs": [],
      "source": [
        "df = df.drop(\"date\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAE60S3iFRpF"
      },
      "source": [
        "---\n",
        "## **Data processing**\n",
        "### Case normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSNd63XFTdlK"
      },
      "outputs": [],
      "source": [
        "# used for all columns to see if null values are present\n",
        "# df.filter(col(\"text\").isNull()).show()\n",
        "\n",
        "df = df.withColumn(\"text\", lower(col(\"text\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJSbjy6kTQEV"
      },
      "source": [
        "### Username and links removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSkQ5dCkGDJX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# twitter ids can contain alphanumeric and '_' characters\n",
        "username_regex = r\"@[A-Za-z0-9_]+\"\n",
        "\n",
        "# http:// / https:// links\n",
        "link_regex1 = r\"https?://[^ ]+\"\n",
        "\n",
        "# www. links\n",
        "link_regex2 = r\"www.[^ ]+\"\n",
        "\n",
        "\n",
        "master_regex = r\"|\".join((username_regex, link_regex1, link_regex2))\n",
        "\n",
        "df = df.withColumn(\"text\", regexp_replace(df.text, master_regex, \"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQNB7bpRaxIG"
      },
      "source": [
        "### Filter out punctual symbols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ILeqT4Ha22N"
      },
      "outputs": [],
      "source": [
        "df = df.withColumn(\"text\", regexp_replace(df.text, \"[^a-zA-Z\\s]\", \"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwH-VfJXU71D"
      },
      "source": [
        "### Trimming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSeeMi1QU-nN"
      },
      "outputs": [],
      "source": [
        "df = df.withColumn(\"text\", trim(col(\"text\")))\n",
        "\n",
        "# extra whitespaces\n",
        "df = df.withColumn(\"text\", trim(regexp_replace(df.text, \" +\", \" \")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We8YznuapL3U"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p6KKbDhpOAF"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(inputCol = \"text\", outputCol = \"tokens\")\n",
        "tokens_df = tokenizer.transform(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOQCWCA3qo0h"
      },
      "source": [
        "### Stopwords removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnulIkVIqrtl"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "stopwords_remover = StopWordsRemover(inputCol = \"tokens\", outputCol = \"terms\")\n",
        "terms_df = stopwords_remover.transform(tokens_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7fcc-7xrMB1"
      },
      "source": [
        "### Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOGvpgdirOOs"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(language = \"english\")\n",
        "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
        "\n",
        "\n",
        "tweets_df = terms_df.withColumn(\"terms_stemmed\", stemmer_udf(\"terms\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLK2YpWdsiaH"
      },
      "source": [
        "### Removal of unnecessary columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUk2wyD9soOX"
      },
      "outputs": [],
      "source": [
        "tweets_df = tweets_df.drop(\"id\", \"text\", \"tokens\", \"terms\")\n",
        "\n",
        "#tweets_df.show(7, truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L6ju2yHRESq"
      },
      "source": [
        "---\n",
        "## **Additional cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJJpggTkRT05"
      },
      "outputs": [],
      "source": [
        "tweets_df.cache()\n",
        "\n",
        "tweets_df = tweets_df.filter(size(tweets_df.terms_stemmed) > 1)\n",
        "\n",
        "sad_tweets = tweets_df.filter(tweets_df.target == 0)\n",
        "happy_tweets = tweets_df.filter(tweets_df.target == 4)\n",
        "\n",
        "sad_tweets_count = sad_tweets.count()\n",
        "happy_tweets_count = happy_tweets.count()\n",
        "\n",
        "print(\"Sad tweets = \", sad_tweets_count)\n",
        "print(\"Happy tweets = \", happy_tweets_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5zA3f9HRXH0"
      },
      "source": [
        "### After the cleaning Sad and Happy tweets are ***unbalanced***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYTHKIqCRgLK"
      },
      "outputs": [],
      "source": [
        "(figure_, axes_) = plt.subplots()\n",
        "\n",
        "y = [sad_tweets_count, happy_tweets_count]\n",
        "axes_.set_ylabel(\"Number of tweets\")\n",
        "\n",
        "x = [\"Sad tweets\", \"Happy tweets\"]\n",
        "\n",
        "axes_.bar(x, y, color = [\"red\", \"green\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBY026hPgd3h"
      },
      "source": [
        "### Balancing dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJygEfWNgmxU"
      },
      "outputs": [],
      "source": [
        "sad_tweets = sad_tweets.select(\"*\").orderBy(rand())\n",
        "sad_tweets = sad_tweets.limit(happy_tweets_count)\n",
        "\n",
        "tweets_df = sad_tweets.union(happy_tweets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY_VvQBrKisT"
      },
      "source": [
        "---\n",
        "## **Word Vector representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5x_pMftKtt2"
      },
      "source": [
        "### Load **GloVe** embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnmMF2HMNcTc"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "embeddings = 100\n",
        "word2vector = None\n",
        "\n",
        "if word2vector == None:\n",
        "  word2vector = api.load(\"glove-twitter-\" + str(embeddings))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3epcP4Vc48E"
      },
      "source": [
        "### Mapping tokens - embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QssWZt9MSB2C"
      },
      "outputs": [],
      "source": [
        "extractor_udf = udf(lambda tokens: [[float(x) for x in word2vector[token]] if token in word2vector else [float(0.0)]*word2vector[\"tweet\"].shape[0] for token in tokens], ArrayType((ArrayType(FloatType()))))\n",
        "\n",
        "avg_embedding_udf = udf(lambda x :[float(y) for y in np.mean(x, axis = 0)])\n",
        "\n",
        "tweets_embedded = tweets_df.withColumn(\"tweet_embeddings\", extractor_udf(\"terms_stemmed\"))\n",
        "tweets_embedded = tweets_embedded.withColumn(\"tweet_embeddings\", avg_embedding_udf(\"tweet_embeddings\"))\n",
        "\n",
        "tweets_embedded.printSchema()\n",
        "tweets_embedded.show(7, truncate = False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbTIOB_ref7U"
      },
      "source": [
        "### Conversion from `ArrayType( ArrayType( Float() ) )` to `VectorUDT()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2t-3-JrtfCtC"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "\n",
        "to_vector_udf = udf(lambda embedded: Vectors.dense(embedded), VectorUDT())\n",
        "\n",
        "tweets_embedded = tweets_embedded.withColumn(\"tweet_embeddings\", to_vector_udf(tweets_embedded.tweet_embeddings))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdjs_CoYOWBu"
      },
      "source": [
        "---\n",
        "## **Train and Test setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxAMJ3ziO3g1"
      },
      "outputs": [],
      "source": [
        "tweets_embedded.cache()\n",
        "\n",
        "(train_df, test_df) = tweets_embedded.randomSplit([0.8, 0.2], seed=1234)\n",
        "\n",
        "train_df_count = train_df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0mbRp_58Bdp"
      },
      "source": [
        "### **Assign weights** to unbalanced columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thWzMiDn8Enr"
      },
      "outputs": [],
      "source": [
        "# calculate the weights for the train set\n",
        "class_weights_train = train_df.groupBy('target').count().withColumn('class_weight_train', 1 / col('count'))\n",
        "hour_weights_train = train_df.groupBy('hour').count().withColumn('hour_weight_train', col('count') / train_df_count)\n",
        "day_weights_train = train_df.groupBy('day_name').count().withColumn('day_weight_train', col('count') / train_df_count)\n",
        "\n",
        "# join the weights\n",
        "train_df_weighted = train_df.join(class_weights_train, 'target', 'left').join(hour_weights_train, 'hour', 'left').join(day_weights_train, 'day_name', 'left')\n",
        "\n",
        "# fill null values in case of missing values\n",
        "train_df_weighted = train_df_weighted.fillna(0.0, subset=['class_weight_train', 'hour_weight_train', 'day_weight_train'])\n",
        "\n",
        "train_df_weighted = train_df_weighted.withColumn('terms_stemmed', col('terms_stemmed'))\n",
        "train_df_weighted = train_df_weighted.withColumn('tweet_embeddings', col('tweet_embeddings'))\n",
        "\n",
        "# delete \"count\" columns\n",
        "#  cannot delate columns with the same name, so I rename them\n",
        "df_cols = train_df_weighted.columns\n",
        "\n",
        "duplicate_col_index = [idx for idx, val in enumerate(df_cols) if val in df_cols[:idx]]\n",
        "\n",
        "cols_to_remove = list()\n",
        "\n",
        "for i in duplicate_col_index:\n",
        "    df_cols[i] = 'count_'+ str(i)\n",
        "    cols_to_remove.append(col(df_cols[i]))\n",
        "\n",
        "train_df_weighted = train_df_weighted.toDF(*df_cols)\n",
        "\n",
        "cols_to_remove.append(col(\"count\"))\n",
        "train_df_weighted = train_df_weighted.drop(*cols_to_remove)\n",
        "\n",
        "#train_df_weighted.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY6nxAOOcoeZ"
      },
      "source": [
        "### Combine weighted columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a41p_MOVcl7_"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler, CountVectorizer\n",
        "\n",
        "train_df_weighted = train_df_weighted.withColumn(\"combined_weight\", train_df_weighted[\"class_weight_train\"] * train_df_weighted[\"hour_weight_train\"] * train_df_weighted[\"day_weight_train\"])\n",
        "test_df = test_df.withColumn(\"combined_weight\", lit(1.0))\n",
        "\n",
        "assembler = VectorAssembler(inputCols = [\"hour\", \"day_name\", \"tweet_embeddings\"], outputCol = \"features\")\n",
        "\n",
        "# before the VectorAssembler I need to handle the string data \"day_name\"\n",
        "days_map = {\"Mon\": 1, \"Tue\": 2, \"Wed\": 3, \"Thu\": 4, \"Fri\": 5, \"Sat\": 6, \"Sun\": 7}\n",
        "\n",
        "convert_day_name_udf = udf(lambda day_name : days_map[day_name], IntegerType())\n",
        "\n",
        "train_df_weighted = train_df_weighted.withColumn(\"day_name\", convert_day_name_udf(train_df_weighted.day_name))\n",
        "test_df = test_df.withColumn(\"day_name\", convert_day_name_udf(test_df.day_name))\n",
        "\n",
        "# apply the VectorAssembler\n",
        "train_df_weighted = assembler.transform(train_df_weighted).select(\"features\", \"target\", \"combined_weight\")\n",
        "test_df = assembler.transform(test_df).select(\"features\", \"target\", \"combined_weight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv3hLOsbqpw8"
      },
      "source": [
        "## **Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ddp3HKNqst5"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(featuresCol='features', labelCol='target', weightCol='combined_weight')\n",
        "lr_model = lr.fit(train_df_weighted)\n",
        "\n",
        "predictions = lr_model.transform(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGRnV9GJAslh"
      },
      "source": [
        "---\n",
        "## Verify data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEO6FUTeAwF9"
      },
      "outputs": [],
      "source": [
        "#tweets_embedded.printSchema()\n",
        "#tweets_embedded.show(10)\n",
        "\n",
        "#print(\"tot = \", tweets_embedded.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpD2gArbjybC"
      },
      "source": [
        "# TODO\n",
        "* **Are useful tokens with len==1 ?**\n",
        "* **chek udf validity for month mapping**\n",
        "* **validation set?**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uY_VvQBrKisT"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyMfX3ELENmS4tXESQ1PPgSZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}