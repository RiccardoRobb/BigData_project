{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNjIHY4SAoMN/E3//0OY/Xy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiccardoRobb/BigData_project/blob/main/BigData_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Twitter sentiment analysis**\n",
        "### BigData 2023 project\n",
        "\n",
        "[@Author](https://github.com/RiccardoRobb): Riccardo Ruberto 1860609"
      ],
      "metadata": {
        "id": "ydnT0-krmCTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Inital configuration**\n",
        "### PySpark installation"
      ],
      "metadata": {
        "id": "TaC0pFVy5ha-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9pfBiCD4wMp",
        "outputId": "c6581332-4a14-44c9-aec4-ef167f1a80b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Useful imports"
      ],
      "metadata": {
        "id": "r5rRWp3R5xKn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DZyT2VD7ZMb6"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark configuration"
      ],
      "metadata": {
        "id": "sMZu4CcD51-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the session\n",
        "conf = SparkConf().\\\n",
        "                set('spark.ui.port', \"8000\").\\\n",
        "                set('spark.executor.memory', '4G').\\\n",
        "                set('spark.driver.memory', '45G').\\\n",
        "                set('spark.driver.maxResultSize', '10G').\\\n",
        "                setAppName(\"Twitter sentiment analysis\").\\\n",
        "                setMaster(\"local[*]\")\n",
        "\n",
        "# Create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "Zc_wOE8Fl7aN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check spark configurations"
      ],
      "metadata": {
        "id": "VJSs9ejr58TM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "-rHsaeM75DvA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "91bf270a-b1bd-41d6-aa69-b517e78f1884"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f325740c8e0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://100b8f9ee09e:8000\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Twitter sentiment analysis</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc._conf.getAll()"
      ],
      "metadata": {
        "id": "hTx8THde5L9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc335b0-4f90-457f-eed7-dafb4d1b6c8b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.app.startTime', '1687364497767'),\n",
              " ('spark.driver.memory', '45G'),\n",
              " ('spark.driver.host', '100b8f9ee09e'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.sql.warehouse.dir', 'file:/content/spark-warehouse'),\n",
              " ('spark.ui.port', '8000'),\n",
              " ('spark.app.submitTime', '1687364497560'),\n",
              " ('spark.driver.maxResultSize', '10G'),\n",
              " ('spark.app.id', 'local-1687364500337'),\n",
              " ('spark.driver.extraJavaOptions',\n",
              "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
              " ('spark.app.name', 'Twitter sentiment analysis'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.submit.pyFiles', ''),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.executor.memory', '4G'),\n",
              " ('spark.ui.showConsoleProgress', 'true'),\n",
              " ('spark.driver.port', '42729'),\n",
              " ('spark.executor.extraJavaOptions',\n",
              "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Load dataset [***Sentiment140***]\n",
        "### Download dataset"
      ],
      "metadata": {
        "id": "qrHRiI0C-jDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/RiccardoRobb/BigData_project/main/Sentiment140.zip\n",
        "\n",
        "! unzip \"./*.zip\" && rm *.zip\n",
        "! mv training.1600000.processed.noemoticon.csv train140.csv"
      ],
      "metadata": {
        "id": "zayTBDQL98iG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8599acf9-d3c0-487d-c7e7-1d5ea3db8ef1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-21 16:21:44--  https://raw.githubusercontent.com/RiccardoRobb/BigData_project/main/Sentiment140.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84855679 (81M) [application/zip]\n",
            "Saving to: ‘Sentiment140.zip’\n",
            "\n",
            "Sentiment140.zip    100%[===================>]  80.92M   312MB/s    in 0.3s    \n",
            "\n",
            "2023-06-21 16:21:44 (312 MB/s) - ‘Sentiment140.zip’ saved [84855679/84855679]\n",
            "\n",
            "Archive:  ./Sentiment140.zip\n",
            "  inflating: training.1600000.processed.noemoticon.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create data frame [***Sentiment140***]\n",
        "1600000 tweets"
      ],
      "metadata": {
        "id": "hAbLzmUGXNu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([ \\\n",
        "    StructField(\"target\",IntegerType(),True), \\\n",
        "    StructField(\"id\",LongType(),True), \\\n",
        "    StructField(\"full_date\",StringType(),True), \\\n",
        "    StructField(\"flag\", StringType(), True), \\\n",
        "    StructField(\"user\", StringType(), True), \\\n",
        "    StructField(\"text\", StringType(), True) \\\n",
        "  ])\n",
        "\n",
        "df = spark.read.csv('./train140.csv', schema=schema, header=\"false\")"
      ],
      "metadata": {
        "id": "e7UAiEc-_GEv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Inital cleaning**\n",
        "### Removal of unnecessary columns"
      ],
      "metadata": {
        "id": "NUGsEwMyXR9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sentiment of the tweet is not affected by the \"user\"\n",
        "df = df.drop(\"user\")\n",
        "\n",
        "# verify \"flag\" utility\n",
        "print(df.select(countDistinct(\"flag\")).collect()[0][0])\n",
        "\n",
        "# \"flag\" has only one value == NO_QUERY, so I delete it\n",
        "df = df.drop(\"flag\")"
      ],
      "metadata": {
        "id": "jfTVRLBRXTDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a43ba5a-fc7c-491f-e97c-cbc1c943aa56"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From ***date*** to ***day_name***; ***hour***; ***date***"
      ],
      "metadata": {
        "id": "efsWAcIVXjxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "months_map = {\"Jan\": \"01\", \"Feb\": \"02\", \"Mar\": \"03\", \"Apr\": \"04\", \"May\": \"05\", \"Jun\": \"06\", \"Jul\": \"07\", \"Aug\": \"08\", \"Sep\": \"09\", \"Oct\": \"10\", \"Nov\": \"11\", \"Dec\": \"12\"}\n",
        "\n",
        "convert_date_udf = udf(lambda month_name : months_map[month_name], StringType())"
      ],
      "metadata": {
        "id": "CnXFsS1rXsjI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_col = pyspark.sql.functions.split(df['full_date'], ' ')\n",
        "\n",
        "df = df.withColumn(\"day_name\", split_col.getItem(0)) \\\n",
        "      .withColumn(\"hour\", split_col.getItem(3)) \\\n",
        "      .withColumn(\"date\", to_date( concat_ws(\"-\", split_col.getItem(2), convert_date_udf(split_col.getItem(1)), split_col.getItem(5)), \"dd-MM-yyyy\"))\n",
        "\n",
        "df = df.drop(\"full_date\")"
      ],
      "metadata": {
        "id": "FcXyz4aKg_ul"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Data analysis**\n",
        "### ***target*** values analisys"
      ],
      "metadata": {
        "id": "6CyYJJi59XKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"target\").distinct().show()\n",
        "# \"target\" value is or 0 or 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1JWoNnM9k4k",
        "outputId": "0264cbf8-507a-4133-9a8c-13ac24c6c6e2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|target|\n",
            "+------+\n",
            "|     4|\n",
            "|     0|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sad_tweets = df.filter(col(\"target\") == 0)\n",
        "happy_tweets = df.filter(col(\"target\") == 4)\n",
        "\n",
        "print(\"Sad tweets = \", sad_tweets.count())\n",
        "print(\"Happy tweets = \", happy_tweets.count())"
      ],
      "metadata": {
        "id": "ehZD6TQNAH6H",
        "outputId": "ea44acf1-3208-47f0-ee14-31eaec367567",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sad tweets =  800000\n",
            "Happy tweets =  800000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sad tweets and happy tweets are balanced."
      ],
      "metadata": {
        "id": "_n13xetpApkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time frame of interest"
      ],
      "metadata": {
        "id": "zW2CBtpkBIGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Min date = \", df.select(min(df.date)).collect()[0][0])\n",
        "print(\"Max date = \", df.select(max(df.date)).collect()[0][0])"
      ],
      "metadata": {
        "id": "fNQmo6WsBQDD",
        "outputId": "a772ed96-4ff2-445a-e962-5d13969ff1c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min date =  2009-04-06\n",
            "Max date =  2009-06-25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The time frame used is too small, datas were collected in 2 months.\n",
        "*Using data column will be useful only if we try to predict tweets written during the [2009-04-06, 2009-06-25] period.*\n",
        "\n",
        "### Better to delete the ***date*** column"
      ],
      "metadata": {
        "id": "5PXQSmfOCezG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(\"date\")"
      ],
      "metadata": {
        "id": "gwpxRRdnDZrI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Data processing**\n",
        "### Case normalization"
      ],
      "metadata": {
        "id": "DAE60S3iFRpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"text\", lower(col(\"text\")))"
      ],
      "metadata": {
        "id": "qSNd63XFTdlK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Username and links removal"
      ],
      "metadata": {
        "id": "qJSbjy6kTQEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# twitter ids can contain alphanumeric and '_' characters\n",
        "username_regex = r\"@[A-Za-z0-9_]+\"\n",
        "\n",
        "# http:// / https:// links\n",
        "link_regex1 = r\"https?://[^ ]+\"\n",
        "\n",
        "# www. links\n",
        "link_regex2 = r\"www.[^ ]+\"\n",
        "\n",
        "\n",
        "master_regex = r\"|\".join((username_regex, link_regex1, link_regex2))\n",
        "\n",
        "df = df.withColumn(\"text\", regexp_replace(df.text, master_regex, \"\"))"
      ],
      "metadata": {
        "id": "gSkQ5dCkGDJX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filter out punctual symbols"
      ],
      "metadata": {
        "id": "tQNB7bpRaxIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"text\", regexp_replace(df.text, \"[^a-zA-Z\\s]\", \"\"))"
      ],
      "metadata": {
        "id": "6ILeqT4Ha22N"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trimming"
      ],
      "metadata": {
        "id": "bwH-VfJXU71D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"text\", trim(col(\"text\")))\n",
        "\n",
        "# extra whitespaces\n",
        "df = df.withColumn(\"text\", trim(regexp_replace(df.text, \" +\", \" \")))"
      ],
      "metadata": {
        "id": "NSeeMi1QU-nN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "We8YznuapL3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(inputCol = \"text\", outputCol = \"tokens\")\n",
        "tokens_df = tokenizer.transform(df)"
      ],
      "metadata": {
        "id": "0p6KKbDhpOAF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopwords removal"
      ],
      "metadata": {
        "id": "UOQCWCA3qo0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "stopwords_remover = StopWordsRemover(inputCol = \"tokens\", outputCol = \"terms\")\n",
        "terms_df = stopwords_remover.transform(tokens_df)"
      ],
      "metadata": {
        "id": "rnulIkVIqrtl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ],
      "metadata": {
        "id": "e7fcc-7xrMB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(language = \"english\")\n",
        "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
        "\n",
        "\n",
        "tweets_df = terms_df.withColumn(\"terms_stemmed\", stemmer_udf(\"terms\"))"
      ],
      "metadata": {
        "id": "VOGvpgdirOOs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removal of unnecessary columns"
      ],
      "metadata": {
        "id": "vLK2YpWdsiaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df = tweets_df.drop(\"id\", \"text\", \"tokens\", \"terms\")\n",
        "\n",
        "tweets_df.show(7, truncate = False)"
      ],
      "metadata": {
        "id": "RUk2wyD9soOX",
        "outputId": "32046c74-34e9-4fc3-be1c-bdab256e98b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+--------+-----------------------------------------------------------------------------------+\n",
            "|target|day_name|hour    |terms_stemmed                                                                      |\n",
            "+------+--------+--------+-----------------------------------------------------------------------------------+\n",
            "|0     |Mon     |22:19:45|[awww, that, bummer, shoulda, got, david, carr, third, day, d]                     |\n",
            "|0     |Mon     |22:19:49|[upset, cant, updat, facebook, text, might, cri, result, school, today, also, blah]|\n",
            "|0     |Mon     |22:19:53|[dive, mani, time, ball, manag, save, rest, go, bound]                             |\n",
            "|0     |Mon     |22:19:57|[whole, bodi, feel, itchi, like, fire]                                             |\n",
            "|0     |Mon     |22:19:57|[behav, im, mad, cant, see]                                                        |\n",
            "|0     |Mon     |22:20:00|[whole, crew]                                                                      |\n",
            "|0     |Mon     |22:20:03|[need, hug]                                                                        |\n",
            "+------+--------+--------+-----------------------------------------------------------------------------------+\n",
            "only showing top 7 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Verify data"
      ],
      "metadata": {
        "id": "DGRnV9GJAslh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df.printSchema()\n",
        "#df.show(10)\n",
        "\n",
        "#print(\"tot = \", df.count())\n",
        "\n",
        "tweets_df.printSchema()\n",
        "tweets_df.show(10)\n",
        "\n",
        "print(\"tot = \", tweets_df.count())"
      ],
      "metadata": {
        "id": "lEO6FUTeAwF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "* **Are useful tokens with len==1 ?**\n",
        "* **chek udf validity for month mapping**"
      ],
      "metadata": {
        "id": "ZpD2gArbjybC"
      }
    }
  ]
}